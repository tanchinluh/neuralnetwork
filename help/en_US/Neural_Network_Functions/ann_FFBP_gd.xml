<?xml version="1.0" encoding="UTF-8"?>

<!--
 *
 * This help file was generated from ann_FFBP_gd.sci using help_from_sci().
 *
 -->

<refentry version="5.0-subset Scilab" xml:id="ann_FFBP_gd" xml:lang="en"
          xmlns="http://docbook.org/ns/docbook"
          xmlns:xlink="http://www.w3.org/1999/xlink"
          xmlns:svg="http://www.w3.org/2000/svg"
          xmlns:ns3="http://www.w3.org/1999/xhtml"
          xmlns:mml="http://www.w3.org/1998/Math/MathML"
          xmlns:db="http://docbook.org/ns/docbook">

  <info>
    <pubdate>$LastChangedDate: 19-Mar-2012 $</pubdate>
  </info>

  <refnamediv>
    <refname>ann_FFBP_gd</refname><refpurpose>ANN FeedForward Backpropagation Gradient Decent training function.</refpurpose>
  </refnamediv>



<refsynopsisdiv>
   <title>Calling Sequence</title>
   <synopsis>
   W = ann_FFBP_gd(P,T,N)
   W = ann_FFBP_gd(P,T,N,af,lr,itermax,mse_min,gd_min)
   
   </synopsis>
</refsynopsisdiv>

<refsection>
   <title>Parameters</title>
   <variablelist>
   <varlistentry><term>P :</term>
      <listitem><para> Training input</para></listitem></varlistentry>
   <varlistentry><term>T :</term>
      <listitem><para> Training target</para></listitem></varlistentry>
   <varlistentry><term>N :</term>
      <listitem><para> Number of Neurons in each layer, incluing Input and output layer</para></listitem></varlistentry>
   <varlistentry><term>af :</term>
      <listitem><para> Activation Function from 1st hidden layer to the output layer</para></listitem></varlistentry>
   <varlistentry><term>lr :</term>
      <listitem><para> learning rate</para></listitem></varlistentry>
   <varlistentry><term>itermax :</term>
      <listitem><para> Maximum epoch for training</para></listitem></varlistentry>
   <varlistentry><term>mse_min :</term>
      <listitem><para> Minumum Error (Performance Goal)</para></listitem></varlistentry>
   <varlistentry><term>gd_min :</term>
      <listitem><para> Minimum Gradient</para></listitem></varlistentry>
   <varlistentry><term>W :</term>
      <listitem><para> Output Weight and bias</para></listitem></varlistentry>
   </variablelist>
</refsection>

<refsection>
   <title>Description</title>
   <para>
This function perform FeedForward Backpropagation with Gradient Decent training algorithm, the most basic training function for FFBP series
   </para>
   <para>
</para>
</refsection>

<refsection>
   <title>Examples</title>
   <programlisting role="example"><![CDATA[
P = [1 2 3 4; 1 2 3 4];
T = [1 2 3 4];
W = ann_FFBP_gd(P,T,[2 3 1]);
y = ann_FFBP_run(P,W)

   ]]></programlisting>
</refsection>

<refsection>
   <title>See also</title>
   <simplelist type="inline">
   <member><link linkend="ann_FFBP_gda">ann_FFBP_gda</link></member>
   <member><link linkend="ann_FFBP_gdm">ann_FFBP_gdm</link></member>
   <member><link linkend="ann_FFBP_gdx">ann_FFBP_gdx</link></member>
   <member><link linkend="ann_FFBP_lm">ann_FFBP_lm</link></member>
   <member><link linkend="ann_FFBP_run">ann_FFBP_run</link></member>
   </simplelist>
</refsection>

<refsection>
   <title>Authors</title>
   <simplelist type="vert">
   <member>Tan C.L.</member>
   </simplelist>
</refsection>
</refentry>
